from openai import OpenAI

# Cliente apuntando a tu Ollama local
client = OpenAI(
    base_url="http://localhost:11434/v1",  # API de Ollama
    api_key="ollama"  # No se valida, solo requerido por la librer√≠a
)

print("üí¨ Generando respuesta...\n")

# Hacemos la petici√≥n en modo streaming
with client.chat.completions.stream(
    model="qwen2.5",
    messages=[
        {"role": "system", "content": "Eres un asistente √∫til que responde en espa√±ol."},
        {"role": "user", "content": "Expl√≠came brevemente c√≥mo funciona Docker Compose."}
    ],
) as stream:
    for event in stream:
        if event.type == "message.delta" and event.delta.content:
            print(event.delta.content, end="", flush=True)
    print()  # Salto de l√≠nea final

